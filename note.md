基础架构的开发环境

* 192.168.16.52
* 192.168.16.53
* 192.168.16.54
* 192.168.16.55

alias sshint="ssh root@118.24.205.100"

sshpass -p 'e!PM@M9sU2086)y' ssh root@192.168.16.52


北京 WiFi

7E78P0AA


邮箱

jira

wiki http://confluence.alauda.cn/pages/viewpage.action?pageId=67561893

gitlab

devops https://acp-hk-build.alauda.cn/console-devops/workspace/tdsql/pipelines/all/tdsql-installer-chart-freeze





本次福利体检请电话：4008100129直接人工客服，报自己身份证号码预约 即可！！开启时间3.10号大家可以周末电话约起来，，本次体检有效时间为：2021.3.10 -5.10 《时间为2个月》请大家安排好自己时间，前往体检！！过期作废噢 [爱心]



https://www.infoq.cn/article/xzx77eee7ih1sdrkm6xj

https://www.reddit.com/r/apachekafka/comments/a5mogm/any_tips_on_ssl_tls/

https://blog.mimacom.com/apache-kafka-with-ssltls-performance/



https://issues.apache.org/jira/browse/KAFKA-2431

https://issues.apache.org/jira/browse/KAFKA-2517






## 相关账号

邮箱账号：zhihu@alauda.io
密码：jiXJO567233

confluence 和 jira 账号：zhihu
confluence 和 jira 密码：jixjo567233

腾讯云
https://acp-hk-build.alauda.cn/console-platform/home/personal-info
账号：zhihu@alauda.io
密码：jiXJO567233



Gitlab 账号
zhihu
jixjo567233

gitlab api 访问账号
zhihu
rcvEkumeJLEk1NF8kkCK

SwitchyOmega
alauda/Tnriw2z267geivn5aLvk

alauda/Lcyxw2z2y0gbbvn7mLvM



git config --global http.proxy http://alauda:Lcyxw2z2y0gbbvn7mLvM@139.186.17.154:52975







http 139.186.17.154:52975



Yuliang_Zhang

bbx09osxozn00n4uvfgtunp9st9taozv



annotations加上skip-sync: 'true'

annotations:



operator.name: ""?





docker login harbor.alauda.cn
admin
raOYiOXaDyGMlLhi



harbor-b.alauda.cn

alaudak8s

ybfzna4qjmdl71t6rg0xpvp8immxpg6f





alaudak8s  oko48xi2bb693ogreibc3xpbuesepo8x
你组内同步下吧



Yueqiang_Hou:wsea8zdumi46anu48lnen641pvz3fo2z



docker login https://index.docker.io/v1/

lanzhiwang

huzhi567233



国内harbor：
地址：https://harbor.alauda.cn/
用户：admin
密码：raOYiOXaDyGMlLhi



http://testlink.alauda.cn

开发自测统一用一个账号。账号：guest 密码：1qaz2wsx#EDC

http://confluence.alauda.cn/display/~xxli/testlink



10.0.130.54:60080/ait/cluster-transformer:v3.5-24-g282dc94

build-harbor.alauda.cn/ait/cluster-transformer:v3.5.3



tar -zcvf pass.tar.gz `git diff 198d68e712fa5913326055915c9fb3843bf84393 21625f5a06c7bef449a6d984ece184bcd6f27670 --name-only`





https://build.alauda.cn/console-devops/workspace/middleware/pipelines/all/rds-operator-with-bundle/rds-operator-with-bundle-f64lq?isMultiBranch=0









/Users/zhh/.minikube/machines/minikube/config.json
 HostOptions --> EngineOptions --> 
  InsecureRegistry: [
            "10.96.0.0/12",
            "192.168.99.0/24"
        ],
        "RegistryMirror": [
            "https://c8it25aj.mirror.aliyuncs.com/",
            "https://9o1kmxjk.mirror.aliyuncs.com"
        ],



int环境：
地址：https://int.alauda.cn
账号：admin@cpaas.io
密码：mai7zeeGoo0ahpi##

sshpass -p 'xSc73WHD4uFBmV' ssh root@192.168.17.24



Ocp 环境

https://new-int.alauda.cn/console-bxliu-acp/admin/domain/list 

ssh core@192.168.31.190

086)yHh3qGCge!PM@M9sU2



ssh -i ~/.ssh/lixiaoxiao.key -p 52022 xxli@118.24.213.153

ssh root@129.28.166.252
07Apples



local-path-provisioner-service-account



local-path-storage



oc adm policy add-scc-to-user hostmount-anyuid -z local-path-provisioner-service-account -n local-path-storage





alias j='sshpass -p 07Apples ssh root@129.28.166.252'

./pull-tag-push.sh new-int.alauda.cn:60080 3rdparty/operators-index:v3.4-pr-319.3



http://10.0.130.233:9000/minio/
minioadmin
minioadmin



https://goproxy.cn,direct

GOPROXY=https://goproxy.cn,direct

GOPROXY="https://athens.alauda.cn"







506073482@qq.com

baotao295



tcpdump -i lo -v -n -mm port 2181



@胡志  学习下吧，你可以把productbaze中的kafka.install设置为false，这样就会把kafka的pod清理掉。然后到对应机器上清理/cpaas/data/kafka* /cpaas/data/zookeper目录，然后再把kafka.install设置为true，同时认证方式改成期望的，base operator就会自动去部署了



kubectl get secret -n cpaas-system acp-config-secret -o yaml



prometheus

https://10.0.129.0/clusters/global/prometheus/graph

https://10.0.129.0/clusters/global/grafana





https://middleware.alauda.cn/clusters/global/prometheus/graph











cat /usr/local/openresty/nginx/conf/policy.new | jq |grep global-alb2-8092-tcp -C10





操作步骤：
1、在页面上上删除集群
2、在每台机器上运行清理脚本
3、在页面上重新加入集群，需要注意填写一些信息，选择 OVN 网络模式



集群地址
10.0.129.27:6443





其他的services可以解析吗，coredns 正常吗， 其他pod里面正常吗？ping下dns呢，在pod里







    ■ Broker Capacity
    ■ Jvm Options
    ■ Java System Properties
    ■ Liveness Probe
    ■ Logging
    ■ Readiness Probe
    ■ Template
    ■ Pod
    ■ Tls Sidecar Container
    ■ Kafka Exporter
    ■ Maintenance Time Windows





```
kubectl delete artifactversions -n cpaas-system `kubectl get artifactversions -A | awk -F ' ' '{print($2)}' | grep rds-operator | grep -v rds-operator.v3.6.16`


kubectl get csv -A | grep rds | awk -F ' ' '{print $1}' | xargs -I {} kubectl -n {} delete csv rds-operator.v3.6.16


kubectl get rdstopic -A | grep -v NAMESPACE |  awk -F ' ' '{print "kubectl -n " $1 " delete rdstopic " $2}' | xargs -t -I {} kubectl {}

kubectl get pods -A | grep my-cluster | awk -F ' ' '{printf("%s delete pods %s\n", $1, $2)}' | xargs -n 4 -t kubectl -n


https://docs.docker.com/registry/spec/api/
[root@dataservice-master ~]# curl -X GET http://192.168.131.207:60080/v2/middleware/kafka/release/0.18.0/operator-bundle/tags/list
{"name":"middleware/kafka/release/0.18.0/operator-bundle","tags":["tdsql-2156.2110081551","tdsql-2156.2110081734","v3.5.0","v3.5.1","v3.5.2","v3.5.3","v3.5.4","v3.5.5","v3.5.6","v3.5.7","v3.5.8","v3.6-0-gcc7b17b5","v3.6-1-g0cb4dc53","v3.6-2-gc09b1815","v3.6-21-g64dec4a5","v3.6-3-g7d80f519","v3.6-42-gddc3dc41c","v3.6-48-gf479acaed","v3.6.0","v3.6.1","v3.6.2","v3.6.3","v3.7-1-g4698e8388","v3.7-10-g5e430e9a3","v3.7-6-g3b605ae6a"]}
[root@dataservice-master ~]#





strimzi/operator:0.18.0
strimzi/kafka:0.18.0-kafka-2.4.0
strimzi/kafka:0.18.0-kafka-2.4.1
strimzi/kafka:0.18.0-kafka-2.5.0
strimzi/jmxtrans:0.18.0
strimzi/kafka-bridge:0.16.0


strimzi/operator:amd64-0.18.0
strimzi/kafka:amd64-0.18.0-kafka-2.4.0
strimzi/kafka:amd64-0.18.0-kafka-2.4.1
strimzi/kafka:amd64-0.18.0-kafka-2.5.0
strimzi/jmxtrans:amd64-0.18.0
strimzi/kafka-bridge:amd64-0.16.0



strimzi/operator:arm64-0.18.0
strimzi/kafka:arm64-0.18.0-kafka-2.4.0
strimzi/kafka:arm64-0.18.0-kafka-2.4.1
strimzi/kafka:arm64-0.18.0-kafka-2.5.0
strimzi/jmxtrans:arm64-0.18.0
strimzi/kafka-bridge:arm64-0.16.0






export DOCKER_BUILDX=buildx
export DOCKER_BUILD_ARGS="--platform linux/arm64"
MVN_ARGS="-Dmaven.javadoc.skip=true -DskipITs -DskipTests" make all


strimzi/test-client:build-kafka-2.5.0 strimzi/test-client:latest strimzi/kafka:build-kafka-2.5.0 strimzi/kafka:latest strimzi/test-client:build-kafka-2.4.1 strimzi/kafka:build-kafka-2.4.1 strimzi/test-client:build-kafka-2.4.0 strimzi/kafka:build-kafka-2.4.0 strimzi/operator:latest strimzi/jmxtrans:latest strimzi/base:latest root/test-client:latest-kafka-2.5.0 root/kafka:latest-kafka-2.5.0 root/test-client:latest-kafka-2.4.1 root/kafka:latest-kafka-2.4.1 root/test-client:latest-kafka-2.4.0 root/kafka:latest-kafka-2.4.0 root/kafka-bridge:latest strimzi/kafka-bridge:latest harbor-b.alauda.cn/tdsql/kafka-bridge:0.16.0 root/operator:latest root/jmxtrans:latest root/jmxtrans:ARM-latest harbor-b.alauda.cn/tdsql/operator:arm-0.18.0 harbor-b.alauda.cn/tdsql/jmxtrans:arm-0.18.0 harbor-b.alauda.cn/tdsql/jmxtrans:latest harbor-b.alauda.cn/tdsql/base:v1 192.168.34.81:60080/tdsql/kafka-bridge:arm-0.16.0 lanzhiwang/kafka-bridge:arm-0.16.0 harbor-b.alauda.cn/tdsql/kafka-bridge:arm-0.16.0 192.168.34.81:60080/tdsql/kafka:arm-0.18.0-kafka-2.5.0 lanzhiwang/kafka:arm-0.18.0-kafka-2.5.0 harbor-b.alauda.cn/tdsql/kafka:arm-0.18.0-kafka-2.5.0 harbor-b.alauda.cn/tdsql/kafka:arm-0.18.0-kafka-2.4.1 192.168.34.81:60080/tdsql/kafka:arm-0.18.0-kafka-2.4.1 lanzhiwang/kafka:arm-0.18.0-kafka-2.4.1 192.168.34.81:60080/tdsql/kafka:arm-0.18.0-kafka-2.4.0 lanzhiwang/kafka:arm-0.18.0-kafka-2.4.0 harbor-b.alauda.cn/tdsql/kafka:arm-0.18.0-kafka-2.4.0 192.168.34.81:60080/tdsql/jmxtrans:arm-0.18.0 lanzhiwang/jmxtrans:arm-0.18.0 192.168.34.81:60080/tdsql/operator:arm-0.18.0 lanzhiwang/operator:arm-0.18.0




helm install mystrimzi ./ --debug --dry-run
helm install --set Cluster=my-kafka mystrimzi ./ --debug --dry-run





helm --set image.repository=192.168.34.233:60080/tdsql --set zookeeper.image.repository=192.168.34.233:60080/tdsql --set kafka.image.repository=192.168.34.233:60080/tdsql --set kafkaConnect.image.repository=192.168.34.233:60080/tdsql --set kafkaConnects2i.image.repository=192.168.34.233:60080/tdsql --set topicOperator.image.repository=192.168.34.233:60080/tdsql --set userOperator.image.repository=192.168.34.233:60080/tdsql --set kafkaInit.image.repository=192.168.34.233:60080/tdsql --set tlsSidecarKafka.image.repository=192.168.34.233:60080/tdsql --set tlsSidecarEntityOperator.image.repository=192.168.34.233:60080/tdsql --set kafkaMirrorMaker.image.repository=192.168.34.233:60080/tdsql --set kafkaBridge.image.repository=192.168.34.233:60080/tdsql --set kafkaExporter.image.repository=192.168.34.233:60080/tdsql --set jmxTrans.image.repository=192.168.34.233:60080/tdsql --set kafkaMirrorMaker2.image.repository=192.168.34.233:60080/tdsql --set cruiseControl.image.repository=192.168.34.233:60080/tdsql --set tlsSidecarCruiseControl.image.repository=192.168.34.233:60080/tdsql install mystrimzi ./ --debug --dry-run


harbor-b.alauda.cn/tdsql/operator:arm-0.18.0
harbor-b.alauda.cn/tdsql/kafka:arm-0.18.0-kafka-2.4.0
harbor-b.alauda.cn/tdsql/kafka:arm-0.18.0-kafka-2.4.1
harbor-b.alauda.cn/tdsql/kafka:arm-0.18.0-kafka-2.5.0
harbor-b.alauda.cn/tdsql/kafka-bridge:arm-0.16.0
harbor-b.alauda.cn/tdsql/jmxtrans:arm-0.18.0

docker tag strimzi/operator:latest harbor-b.alauda.cn/tdsql/operator:arm-0.18.0

docker tag strimzi/kafka:build-kafka-2.4.0 harbor-b.alauda.cn/tdsql/kafka:arm-0.18.0-kafka-2.4.0

docker tag strimzi/kafka:build-kafka-2.4.1 harbor-b.alauda.cn/tdsql/kafka:arm-0.18.0-kafka-2.4.1

docker tag strimzi/kafka:build-kafka-2.5.0 harbor-b.alauda.cn/tdsql/kafka:arm-0.18.0-kafka-2.5.0

docker tag strimzi/kafka-bridge:latest harbor-b.alauda.cn/tdsql/kafka-bridge:arm-0.16.0

docker tag strimzi/jmxtrans:latest harbor-b.alauda.cn/tdsql/jmxtrans:arm-0.18.0


docker tag strimzi/operator:latest lanzhiwang/operator:arm-0.18.0

docker tag strimzi/kafka:build-kafka-2.4.0 lanzhiwang/kafka:arm-0.18.0-kafka-2.4.0

docker tag strimzi/kafka:build-kafka-2.4.1 lanzhiwang/kafka:arm-0.18.0-kafka-2.4.1

docker tag strimzi/kafka:build-kafka-2.5.0 lanzhiwang/kafka:arm-0.18.0-kafka-2.5.0

docker tag strimzi/kafka-bridge:latest lanzhiwang/kafka-bridge:arm-0.16.0

docker tag strimzi/jmxtrans:latest lanzhiwang/jmxtrans:arm-0.18.0



docker tag harbor-b.alauda.cn/tdsql/operator:arm-0.18.0 192.168.34.233:60080/tdsql/operator:arm-0.18.0
docker tag harbor-b.alauda.cn/tdsql/kafka:arm-0.18.0-kafka-2.4.0 192.168.34.233:60080/tdsql/kafka:arm-0.18.0-kafka-2.4.0
docker tag harbor-b.alauda.cn/tdsql/kafka:arm-0.18.0-kafka-2.4.1 192.168.34.233:60080/tdsql/kafka:arm-0.18.0-kafka-2.4.1
docker tag harbor-b.alauda.cn/tdsql/kafka:arm-0.18.0-kafka-2.5.0 192.168.34.233:60080/tdsql/kafka:arm-0.18.0-kafka-2.5.0
docker tag harbor-b.alauda.cn/tdsql/kafka-bridge:arm-0.16.0 192.168.34.233:60080/tdsql/kafka-bridge:arm-0.16.0
docker tag harbor-b.alauda.cn/tdsql/jmxtrans:arm-0.18.0 192.168.34.233:60080/tdsql/jmxtrans:arm-0.18.0


192.168.34.233:60080/tdsql/operator:arm-0.18.0
192.168.34.233:60080/tdsql/kafka:arm-0.18.0-kafka-2.4.0
192.168.34.233:60080/tdsql/kafka:arm-0.18.0-kafka-2.4.1
192.168.34.233:60080/tdsql/kafka:arm-0.18.0-kafka-2.5.0
192.168.34.233:60080/tdsql/kafka-bridge:arm-0.16.0
192.168.34.233:60080/tdsql/jmxtrans:arm-0.18.0




docker tag harbor-b.alauda.cn/tdsql/operator:arm-0.18.0 192.168.34.81:60080/tdsql/operator:arm-0.18.0
docker tag harbor-b.alauda.cn/tdsql/kafka:arm-0.18.0-kafka-2.4.0 192.168.34.81:60080/tdsql/kafka:arm-0.18.0-kafka-2.4.0
docker tag harbor-b.alauda.cn/tdsql/kafka:arm-0.18.0-kafka-2.4.1 192.168.34.81:60080/tdsql/kafka:arm-0.18.0-kafka-2.4.1
docker tag harbor-b.alauda.cn/tdsql/kafka:arm-0.18.0-kafka-2.5.0 192.168.34.81:60080/tdsql/kafka:arm-0.18.0-kafka-2.5.0
docker tag harbor-b.alauda.cn/tdsql/kafka-bridge:arm-0.16.0 192.168.34.81:60080/tdsql/kafka-bridge:arm-0.16.0
docker tag harbor-b.alauda.cn/tdsql/jmxtrans:arm-0.18.0 192.168.34.81:60080/tdsql/jmxtrans:arm-0.18.0


helm --set image.repository=192.168.34.81:60080/tdsql --set zookeeper.image.repository=192.168.34.81:60080/tdsql --set kafka.image.repository=192.168.34.81:60080/tdsql --set kafkaConnect.image.repository=192.168.34.81:60080/tdsql --set kafkaConnects2i.image.repository=192.168.34.81:60080/tdsql --set topicOperator.image.repository=192.168.34.81:60080/tdsql --set userOperator.image.repository=192.168.34.81:60080/tdsql --set kafkaInit.image.repository=192.168.34.81:60080/tdsql --set tlsSidecarKafka.image.repository=192.168.34.81:60080/tdsql --set tlsSidecarEntityOperator.image.repository=192.168.34.81:60080/tdsql --set kafkaMirrorMaker.image.repository=192.168.34.81:60080/tdsql --set kafkaBridge.image.repository=192.168.34.81:60080/tdsql --set kafkaExporter.image.repository=192.168.34.81:60080/tdsql --set jmxTrans.image.repository=192.168.34.81:60080/tdsql --set kafkaMirrorMaker2.image.repository=192.168.34.81:60080/tdsql --set cruiseControl.image.repository=192.168.34.81:60080/tdsql --set tlsSidecarCruiseControl.image.repository=192.168.34.81:60080/tdsql install mystrimzi ./ --debug --dry-run



docker inspect harbor-b.alauda.cn/3rdparty/operator@sha256:2857eec4b4c5eca0fbe342fa37497dcc9eee6fdbc06db53910c92136176ed34d

docker inspect 192.168.34.81:60080/tdsql/operator:arm-0.18.0

docker inspect harbor-b.alauda.cn/tdsql/operator:arm-0.18.0



docker images harbor-b.alauda.cn/3rdparty/operator@sha256:2857eec4b4c5eca0fbe342fa37497dcc9eee6fdbc06db53910c92136176ed34d harbor-b.alauda.cn/tdsql/operator:arm-0.18.0


docker manifest create harbor-b.alauda.cn/tdsql/operator:0.18.0 harbor-b.alauda.cn/3rdparty/operator@sha256:2857eec4b4c5eca0fbe342fa37497dcc9eee6fdbc06db53910c92136176ed34d harbor-b.alauda.cn/tdsql/operator:arm-0.18.0

docker manifest inspect harbor-b.alauda.cn/tdsql/operator:0.18.0

[root@hudi-arm-master-0001 ~]# docker manifest push  harbor-b.alauda.cn/tdsql/operator:0.18.0
Pushed ref harbor-b.alauda.cn/tdsql/operator@sha256:2857eec4b4c5eca0fbe342fa37497dcc9eee6fdbc06db53910c92136176ed34d with digest: sha256:2857eec4b4c5eca0fbe342fa37497dcc9eee6fdbc06db53910c92136176ed34d
sha256:404b3ce3b754cbfb7616110e5e2098dd724f625055c6ccb34aa2081166d73401
[root@hudi-arm-master-0001 ~]#



docker tag harbor-b.alauda.cn/3rdparty/operator@sha256:2857eec4b4c5eca0fbe342fa37497dcc9eee6fdbc06db53910c92136176ed34d 192.168.34.81:60080/3rdparty/operator@sha256:2857eec4b4c5eca0fbe342fa37497dcc9eee6fdbc06db53910c92136176ed34d


docker tag harbor-b.alauda.cn/tdsql/operator:arm-0.18.0 192.168.34.81:60080/tdsql/operator:arm-0.18.0



    
####################



$ kubectl create ns kafka
namespace/kafka created

$ helm --set image.repository=192.168.34.81:60080/tdsql --set zookeeper.image.repository=192.168.34.81:60080/tdsql --set kafka.image.repository=192.168.34.81:60080/tdsql --set kafkaConnect.image.repository=192.168.34.81:60080/tdsql --set kafkaConnects2i.image.repository=192.168.34.81:60080/tdsql --set topicOperator.image.repository=192.168.34.81:60080/tdsql --set userOperator.image.repository=192.168.34.81:60080/tdsql --set kafkaInit.image.repository=192.168.34.81:60080/tdsql --set tlsSidecarKafka.image.repository=192.168.34.81:60080/tdsql --set tlsSidecarEntityOperator.image.repository=192.168.34.81:60080/tdsql --set kafkaMirrorMaker.image.repository=192.168.34.81:60080/tdsql --set kafkaBridge.image.repository=192.168.34.81:60080/tdsql --set kafkaExporter.image.repository=192.168.34.81:60080/tdsql --set jmxTrans.image.repository=192.168.34.81:60080/tdsql --set kafkaMirrorMaker2.image.repository=192.168.34.81:60080/tdsql --set cruiseControl.image.repository=192.168.34.81:60080/tdsql --set tlsSidecarCruiseControl.image.repository=192.168.34.81:60080/tdsql -n kafka install mystrimzi ./
NAME: mystrimzi
LAST DEPLOYED: Fri Dec 25 13:42:30 2020
NAMESPACE: kafka
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
Thank you for installing strimzi-kafka-operator-arm-0.18.0

To create a Kafka cluster refer to the following documentation.

https://strimzi.io/docs/0.18.0/#kafka-cluster-str

$ helm list -n kafka
NAME     	NAMESPACE	REVISION	UPDATED                                	STATUS  	CHART                            	APP VERSION
mystrimzi	kafka    	1       	2020-12-25 13:42:30.169548335 +0800 CST	deployed	strimzi-kafka-operator-arm-0.18.0	0.18.0

$ kubectl -n kafka get pods
NAME                                        READY   STATUS    RESTARTS   AGE
strimzi-cluster-operator-56d865b4d7-nnf2l   1/1     Running   0          43s


[root@hudi-arm-master-0001 chart]# kubectl -n kafka get pvc
NAME                                 STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
data-acp-kafka-cluster-kafka-0       Bound    pvc-30c5a1a2-e643-46b3-b4ad-6781195124f5   1Gi        RWO            local-path     6m15s
data-acp-kafka-cluster-kafka-1       Bound    pvc-69b07fd6-2ebf-4146-b616-7794acf29cce   1Gi        RWO            local-path     6m15s
data-acp-kafka-cluster-kafka-2       Bound    pvc-9a8d9a37-d2a5-4197-8469-31a394c54bb1   1Gi        RWO            local-path     6m15s
data-acp-kafka-cluster-zookeeper-0   Bound    pvc-3e8fdee9-c905-4e42-b73b-feceb44bb9d9   1Gi        RWO            local-path     7m36s
data-acp-kafka-cluster-zookeeper-1   Bound    pvc-19b70e64-85b7-4e29-84ac-f8f54f7257ca   1Gi        RWO            local-path     7m36s
data-acp-kafka-cluster-zookeeper-2   Bound    pvc-fcf8bf88-6ff1-4317-a93b-fead9a2c54c5   1Gi        RWO            local-path     7m36s
[root@hudi-arm-master-0001 chart]#


kubectl get pv pvc-30c5a1a2-e643-46b3-b4ad-6781195124f5 pvc-69b07fd6-2ebf-4146-b616-7794acf29cce pvc-9a8d9a37-d2a5-4197-8469-31a394c54bb1 pvc-3e8fdee9-c905-4e42-b73b-feceb44bb9d9 pvc-19b70e64-85b7-4e29-84ac-f8f54f7257ca pvc-fcf8bf88-6ff1-4317-a93b-fead9a2c54c5


[root@hudi-arm-master-0001 chart]# kubectl get pv pvc-30c5a1a2-e643-46b3-b4ad-6781195124f5 pvc-69b07fd6-2ebf-4146-b616-7794acf29cce pvc-9a8d9a37-d2a5-4197-8469-31a394c54bb1 pvc-3e8fdee9-c905-4e42-b73b-feceb44bb9d9 pvc-19b70e64-85b7-4e29-84ac-f8f54f7257ca pvc-fcf8bf88-6ff1-4317-a93b-fead9a2c54c5
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                                      STORAGECLASS   REASON   AGE
pvc-30c5a1a2-e643-46b3-b4ad-6781195124f5   1Gi        RWO            Retain           Bound    kafka/data-acp-kafka-cluster-kafka-0       local-path              10m
pvc-69b07fd6-2ebf-4146-b616-7794acf29cce   1Gi        RWO            Retain           Bound    kafka/data-acp-kafka-cluster-kafka-1       local-path              10m
pvc-9a8d9a37-d2a5-4197-8469-31a394c54bb1   1Gi        RWO            Retain           Bound    kafka/data-acp-kafka-cluster-kafka-2       local-path              10m
pvc-3e8fdee9-c905-4e42-b73b-feceb44bb9d9   1Gi        RWO            Retain           Bound    kafka/data-acp-kafka-cluster-zookeeper-0   local-path              11m
pvc-19b70e64-85b7-4e29-84ac-f8f54f7257ca   1Gi        RWO            Retain           Bound    kafka/data-acp-kafka-cluster-zookeeper-1   local-path              11m
pvc-fcf8bf88-6ff1-4317-a93b-fead9a2c54c5   1Gi        RWO            Retain           Bound    kafka/data-acp-kafka-cluster-zookeeper-2   local-path              11m
[root@hudi-arm-master-0001 chart]#



apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: data-acp-kafka-cluster-kafka-0
  namespace: kafka
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
  storageClassName: local-path
  volumeMode: Filesystem
  volumeName: pvc-30c5a1a2-e643-46b3-b4ad-6781195124f5

---

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: data-acp-kafka-cluster-kafka-1
  namespace: kafka
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
  storageClassName: local-path
  volumeMode: Filesystem
  volumeName: pvc-69b07fd6-2ebf-4146-b616-7794acf29cce

---

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: data-acp-kafka-cluster-kafka-2
  namespace: kafka
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
  storageClassName: local-path
  volumeMode: Filesystem
  volumeName: pvc-9a8d9a37-d2a5-4197-8469-31a394c54bb1

---

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: data-acp-kafka-cluster-zookeeper-0
  namespace: kafka
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
  storageClassName: local-path
  volumeMode: Filesystem
  volumeName: pvc-3e8fdee9-c905-4e42-b73b-feceb44bb9d9

---

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: data-acp-kafka-cluster-zookeeper-1
  namespace: kafka
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
  storageClassName: local-path
  volumeMode: Filesystem
  volumeName: pvc-19b70e64-85b7-4e29-84ac-f8f54f7257ca

---

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: data-acp-kafka-cluster-zookeeper-2
  namespace: kafka
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
  storageClassName: local-path
  volumeMode: Filesystem
  volumeName: pvc-fcf8bf88-6ff1-4317-a93b-fead9a2c54c5

---










kafka-producer-perf-test.sh --num-records 500 --topic my-topic --throughput -1 --record-size 1000 --producer-props bootstrap.servers=192.168.34.81:30011


kafka-console-consumer.sh --bootstrap-server 192.168.34.81:30011 --topic my-topic --from-beginning --group my-group

kafka-consumer-groups.sh --bootstrap-server 192.168.34.81:30011 --list

[root@hudi-arm-master-0001 chart]# kafka-consumer-groups.sh --bootstrap-server 192.168.34.81:30011 --describe --group my-group

Consumer group 'my-group' has no active members.

GROUP           TOPIC           PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG             CONSUMER-ID     HOST            CLIENT-ID
my-group        my-topic        0          144             144             0               -               -               -
my-group        my-topic        1          148             148             0               -               -               -
my-group        my-topic        2          208             208             0               -               -               -
[root@hudi-arm-master-0001 chart]#



apiVersion: kafka.strimzi.io/v1beta1
kind: KafkaTopic
metadata:
  name: consumer-offsets---84e7a678d08f4bd226872e5cdd4eb527fadc1c6a
  namespace: kafka
spec:
  config:
    cleanup.policy: compact
    compression.type: producer
    segment.bytes: "104857600"
  partitions: 50
  replicas: 3
  topicName: __consumer_offsets

---

apiVersion: kafka.strimzi.io/v1beta1
kind: KafkaTopic
metadata:
  name: my-topic
  namespace: kafka
spec:
  partitions: 3
  replicas: 3








1、删除命名空间

[root@hudi-arm-master-0001 chart]# kubectl delete ns kafka
namespace "kafka" deleted
[root@hudi-arm-master-0001 chart]#

2、确定 pv 存在且处于 Released 状态

[root@hudi-arm-master-0001 chart]# kubectl get pv pvc-30c5a1a2-e643-46b3-b4ad-6781195124f5 pvc-69b07fd6-2ebf-4146-b616-7794acf29cce pvc-9a8d9a37-d2a5-4197-8469-31a394c54bb1 pvc-3e8fdee9-c905-4e42-b73b-feceb44bb9d9 pvc-19b70e64-85b7-4e29-84ac-f8f54f7257ca pvc-fcf8bf88-6ff1-4317-a93b-fead9a2c54c5
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS     CLAIM                                      STORAGECLASS   REASON   AGE
pvc-30c5a1a2-e643-46b3-b4ad-6781195124f5   1Gi        RWO            Retain           Released   kafka/data-acp-kafka-cluster-kafka-0       local-path              22m
pvc-69b07fd6-2ebf-4146-b616-7794acf29cce   1Gi        RWO            Retain           Released   kafka/data-acp-kafka-cluster-kafka-1       local-path              22m
pvc-9a8d9a37-d2a5-4197-8469-31a394c54bb1   1Gi        RWO            Retain           Released   kafka/data-acp-kafka-cluster-kafka-2       local-path              21m
pvc-3e8fdee9-c905-4e42-b73b-feceb44bb9d9   1Gi        RWO            Retain           Released   kafka/data-acp-kafka-cluster-zookeeper-0   local-path              23m
pvc-19b70e64-85b7-4e29-84ac-f8f54f7257ca   1Gi        RWO            Retain           Released   kafka/data-acp-kafka-cluster-zookeeper-1   local-path              23m
pvc-fcf8bf88-6ff1-4317-a93b-fead9a2c54c5   1Gi        RWO            Retain           Released   kafka/data-acp-kafka-cluster-zookeeper-2   local-path              23m
[root@hudi-arm-master-0001 chart]#


3、kubectl create ns kafka

4、kubectl -n kafka apply -f  pvc.yaml

5、重新使用 helm 安装 operator 

6、恢复 topic

7、恢复 kafka 集群

[root@hudi-arm-master-0001 chart]# kafka-consumer-groups.sh --bootstrap-server 192.168.34.81:31546 --describe --group my-group

Consumer group 'my-group' has no active members.

GROUP           TOPIC           PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG             CONSUMER-ID     HOST            CLIENT-ID
my-group        my-topic        0          144             144             0               -               -               -
my-group        my-topic        1          148             148             0               -               -               -
my-group        my-topic        2          208             208             0               -               -               -
[root@hudi-arm-master-0001 chart]#






```





cpaas-kafka:9092




使用 http 协议

```
$ git config --system --list
fatal: unable to read config file '/etc/gitconfig': No such file or directory


$ git config --global --list
fatal: unable to read config file '/Users/huzhi/.gitconfig': No such file or directory


$ git config --local --list
fatal: --local can only be used inside a git repository


$ git config --local --list
core.repositoryformatversion=0
core.filemode=true
core.bare=false
core.logallrefupdates=true
core.ignorecase=true
core.precomposeunicode=true
user.email=hzhilamp@163.com
user.name=lanzhiwang
remote.origin.url=git@github.com:lanzhiwang/strimzi-0.18.0.git
remote.origin.fetch=+refs/heads/*:refs/remotes/origin/*
branch.main.remote=origin
branch.main.merge=refs/heads/main

git config --global http.proxy http://127.0.0.1:1087
git config --global https.proxy https://127.0.0.1:1087

$ git config --global http.proxy http://alauda:Tnriw2z267geivn5aLvk@139.186.17.154:52975
$ git config --global --list
http.proxy=http://alauda:Tnriw2z267geivn5aLvk@139.186.17.154:52975

$ cat /Users/huzhi/.gitconfig
[http]
    proxy = http://alauda:Tnriw2z267geivn5aLvk@139.186.17.154:52975

$ export GIT_SSL_NO_VERIFY=true

$ git config --global --unset http.proxy
$ git config --global --list
$ cat /Users/huzhi/.gitconfig


git config user.email "hzhilamp@163.com"
git config user.name "lanzhiwang"

git config user.name "胡志"
git config user.email "zhihu@alauda.io"
  
```



Jenkins  配置

```json
[
    config: [
        agent: maven3.5,
        folder:.,
        scm: [
            credentials:devops-alauda-gitlab,
            submodules: false
        ],
        sonar: [
            binding:sonarqube,
            enabled: true
        ],
        pod: [
            yaml:
        ],
        chart: [
            name: ,
            component:
        ],
        chartX: [
            enabled: false
        ],
        docker: [
            repository: ,
            credentials:alaudak8s,
            context:.,
            dockerfile:Dockerfile,
            armBuild: true,
            disableArmBuildOnPR: false,
            enabled: true,
            scan: true
        ],
        operator: [
            hub_project:common,
            hub_pipeline:operators-index,
            bundle: [
                name: ,
                script:make bundle,
                dockerfile:bundle.Dockerfile,
                repository:
            ],
            enabled: false
        ],
        sec: [
            enabled: true,
            block: false,
            lang: ,
            scanMod: 1,
            customOpts:
        ],
        notification: [
            :
        ]
    ],
    steps: [
        [
            name:tool,
            container:java,
            commands: [./Jenkinsfile-script/install_tool.sh]
        ],
        [
            name:Build,
            container:java,
            commands: [MVN_ARGS="-Dmaven.javadoc.skip=true -DskipITs -DskipTests" make all]
        ]
    ],
    yaml: ,
    env: [
        :
    ],
    endSteps: [],
    post: [
        always: []
    ]
]

```
















## 产品的历史

* 2017年5月 **Alauda EE 1.X（ACE 1.X）**
* 2018年10月 **ACE 2.X**
* 2018年11月开始开发 **ACP 1.0**，基于ACP 1.X，增加了三个新的产品：**DevOps**、**ASM(service mesh)**、**AML(machine learning)**。
* 2019年6月结合 ACE 2.X 和 ACP 1.X 的优势，发布了 **ACP 2.0** 产品。平台采用同 ACP 1.X 一样的 k8s 扩展机制实现，并支持多集群多租户的能力。容器和 DevOps 功能上和 ACE 2.X 对齐，同时还将 ASM、AML 迁移到ACP 2.X 上。在 ACP 2.X 的开发过程中，同时开始了和腾讯TKE团队开发的 **TKE 3.0** 产品的融合。
* 2019年9月发布 ACP 2.x 和 TKE 3.0 的融合产品 **TKE for Alauda 2.3（即ACP 2.3）**。该产品的集群管理、集群插件管理采用了 TKE 3.0 的模块，其它模块仍然使用 ACP 2.X。后来 TKE for Alauda 改名为 **TKE企业版**，内部还习惯称 ACP。
* 2019年底 TKE企业版 和 **TSF、TDSQL、CSP** 等腾讯产品开始进行对接。同时开始 **TKE PaaS** 的开发，TKE PaaS 是以 TKE 企业版为底座，再加上若干腾讯产品（操作系统、数据库、存储、微服务等）后形成的较完整的 PaaS 平台。TKE PaaS 内部有时会习惯称为 **ACE 3.0**。
* 2020年3月发布 **TKE企业版2.9（即ACP 2.12）** 以及 **TKE PaaS beta版**，包含 TSF、TDSQL、CSP、Ti-Matrix等腾讯产品的集成。
* 2020年6月发布 **TKE企业版2.12（即ACP 2.12）** 以及 **TKE PaaS 正式版**，包含 TSF、TDSQL、CSP、Redis、CMQ、Coding、Ti-Matrix 等腾讯产品的集成。



## 腾讯跳板机

```
跳板机登陆方法：http://confluence.alauda.cn/pages/viewpage.action?pageId=27172708
# Tencent Yun Chongqing springboard
# jumpserver IP:118.24.213.153
# jumpserver login user: zhihu
# jumpserver port:52022
# jumpserver login key
#####################################
-----BEGIN RSA PRIVATE KEY-----
MIIEpAIBAAKCAQEA2mSPIBBvCu7TgdHeB0LOWxynbocQ5GEzczKh2N0Egft4ICnH
8D9yccSJwFDhloXCxlR0x48tAbNXmnfwox7bLCL9IHBUVDrGcNIRKTKAzFlGxSsm
gdIzc3P85W9/VwQCPeos062chffDZbCAO0608K+0TI2ULJDJ7efnw8Hc+5qTmpk/
JYfmoTNYWlgF52X+sTWvxH2b5ZKXBEDGDSRGWK+ztazwj2HbLeyULijxHq/TF9jf
48ebCamn9Vj1/7ZcyG24jj383+0V+l3im+P0QTQQqeGsAiXLNRtckOdo3G7v1X06
4m7CQ8VJWWwKkDbYob7CZeaePAT1iH+JK3y5/wIDAQABAoIBAQC+yZyzkUfA0FVc
EyCZTUaDGCw5Bau/KB9PYws7LhIzD6Goz3dIrdToCJ+ir8XzvpUiuENw1uallqxQ
WLmTd3BXRZXG0fsJvnL/urSdPe6BFvkZZJH2VdD26nwjX91qAimHN13D6uZTrtg3
rRLJPQAbSBQF0KWCrVaLDM5PGLxZ29n3AidWkjGR+mQiR6l23uCHNwj4GEF+CXrj
GKhsHsj687j8M9klycNsNDSvtDwWOtp+M6BPSaXlnrY+ycLhDJ5LS+2uL9j8tvev
eWXxlFfLL0fKHcs4yZkfpJWOzj8P1MKvNzbAYWUYgIaK0LSJNEIfwpp6nC1t4foe
yD0YJ2bZAoGBAPTaOuYI797U6CMnLO0jue0Qp9uMi9zsYCyxrdAPowbk+9mgO8XI
xEQ/u42bXQB0U1Q7lQOgWZfu6Ue3fp+CNFhlgFkcP/eenTB5iNOb/VsFVuDphkKH
QLbYJbSWuYdDlNOoPxLKgjS1T5l6yo0Ck7XtxG0jGj5/uDSQrpfHbdUbAoGBAORV
8LC9FAxN7aFXehSSgNjBWDiD+F/aOCXhiiHPj30fV6JoDTQNFb+A/2A+L18DOvPW
AlNXVQC7xIWCIqQ+dGebVS0R94y2jm+YbWfXcmh2Tfx2fwZrISqW2zmU4qsxhOfU
k2ULb0/Y784AMkWvCzpUaYkFS8gFdEtQS1WKAlDtAoGAXzye2iixvDeNz1aGh/p0
b/whfijtodGjGt9FXv8mByF7wEst1KFhjbZIaiz7AJk+bC38qPtuvcTkocuCieJo
H9XjFUYCr3rXYypyiPRMmGG8SCEs4qWfCz+JcvOJWE52DdmMJu/zszKusmDrdeuB
rqq700NrCtI8wN1hu5GLa+8CgYEA2ZHmIZJY+wx6RIVlBxs9+MvqcxeU4Ei/vaC2
DVeIozHtQAwjoJhjQ7H4JM28N62NS/B9EMqjbWp9bLW+qn/0TRDezW5UUllVSZKV
lR/enRk1YD3M9eG4natXQvvSLEuoF3sf42VM8GmGvuTDAlEzwqXSVcSdG//Oe0EM
N3qkkFUCgYBbcj7y9X9qZIDHsDeYBWH2Xp0kNZPeOOwAbHoZV3HYYIc2zpeB2GSJ
8DUTwG7OpA8y0aPJRRUdVSi3xBRZtd1LyzX+DD03tPxKZ8AU2Ydh4fetuD4LzxQC
OYhd0IksT1TU6a9l8WVDRBSS+eQFQmuEetFD0p7J0xE2Qtg9ZSet/Q==
-----END RSA PRIVATE KEY-----

# login: ssh root@public_ip

# 登录跳板机
ssh -i ~/.ssh/lqy_tencent_jumpserver.pem zhihu@118.24.213.153 -p 52022


[zhihu@jumpserver ~]$
[zhihu@jumpserver ~]$ ssh root@139.186.122.125
Warning: Permanently added '139.186.122.125' (ECDSA) to the list of known hosts.
Last login: Mon Sep 21 14:53:24 2020 from 61.148.199.18
[root@mw-init ~]#


Host *
    ServerAliveInterval 10
    TCPKeepAlive yes
    ControlPersist yes
    ControlMaster auto
    ControlPath ~/.ssh/master_%r_%h_%p

ssh -i ~/.ssh/lqy_tencent_jumpserver.pem zhihu@118.24.213.153 -p 52022
Host tencent_jumper
    HostName 118.24.213.153
    User zhihu
    Port 52022
    IdentityFile ~/.ssh/lqy_tencent_jumpserver.pem
Host global-init
    HostName 139.186.122.125
    User root
    IdentityFile ~/.ssh/lqy
    ProxyJump tencent_jumper
    
    
```

上手项目描述：编写一个Operator，实现对于指定环境变量内的资源监控，具体要求如下：
1. 基于Operator-SDK来实现operator
2. 每隔1分钟执行一次刷新操作
3. 每次刷新的时候，在stdout打印下列资源的信息：
    deployment, statefulset, secret, configmap, pod, service, endpoint
    每种信息需要打印name, namespace, kind, status以及其他你认为适合的信息
4. 编写chart来部署这个operator
5. 设置流水线来支持这个项目的CI

疑问：
1. 如何指定环境变量，也就是如何输入？

CRD
CR









kafka 官方 operator 提交历史

https://github.com/banzaicloud/kafka-operator/commits/master?after=5a7218622140ee10e9872213316d3056b8f2c2e3+629&branch=master







* dlv
* go-outline
* gocode
* godef
* goimports
* gomodifytags
* goplay
* gorename
* gotests
* impl
* fillstruct
* go-symbols
* gocode-gomod
* godoctor
* golint
* gopkgs
* gopls
* goreturns
* guru











global.images.imoocpodOperator.tag=origin_master-b18-7018120.0





blob:https://harbor-b.alauda.cn/9b6965d5-d5ea-43d4-bce3-c0e7984c1cd9







```
---
# Source: mychart/templates/service_account.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: imooc-operator
---
# Source: mychart/templates/k8s.imooc.com_imoocpods_crd.yaml
apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: imoocpods.k8s.imooc.com
spec:
  group: k8s.imooc.com
  names:
    kind: ImoocPod
    listKind: ImoocPodList
    plural: imoocpods
    singular: imoocpod
  scope: Namespaced
  subresources:
    status: {}
  validation:
    openAPIV3Schema:
      description: ImoocPod is the Schema for the imoocpods API
      properties:
        apiVersion:
          description: 'APIVersion defines the versioned schema of this representation
            of an object. Servers should convert recognized schemas to the latest
            internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources'
          type: string
        kind:
          description: 'Kind is a string value representing the REST resource this
            object represents. Servers may infer this from the endpoint the client
            submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds'
          type: string
        metadata:
          type: object
        spec:
          description: ImoocPodSpec defines the desired state of ImoocPod
          properties:
            replicas:
              description: 'INSERT ADDITIONAL SPEC FIELDS - desired state of cluster
                Important: Run "operator-sdk generate k8s" to regenerate code after
                modifying this file Add custom validation using kubebuilder tags:
                https://book-v1.book.kubebuilder.io/beyond_basics/generating_crd.html'
              type: integer
            size:
              type: integer
            watched_namespace:
              type: string
          required:
          - replicas
          - size
          - watched_namespace
          type: object
        status:
          description: ImoocPodStatus defines the observed state of ImoocPod
          properties:
            podNames:
              items:
                type: string
              type: array
            replicas:
              description: 'INSERT ADDITIONAL STATUS FIELD - define observed state
                of cluster Important: Run "operator-sdk generate k8s" to regenerate
                code after modifying this file Add custom validation using kubebuilder
                tags: https://book-v1.book.kubebuilder.io/beyond_basics/generating_crd.html'
              type: integer
            size:
              type: integer
            watched_namespace:
              type: string
          required:
          - podNames
          - replicas
          - size
          - watched_namespace
          type: object
      type: object
  version: v1alpha1
  versions:
  - name: v1alpha1
    served: true
    storage: true
---
# Source: mychart/templates/role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  creationTimestamp: null
  name: imooc-operator
rules:
- apiGroups:
  - ""
  resources:
  - pods
  - services
  - services/finalizers
  - endpoints
  - persistentvolumeclaims
  - events
  - configmaps
  - secrets
  verbs:
  - create
  - delete
  - get
  - list
  - patch
  - update
  - watch
- apiGroups:
  - apps
  resources:
  - deployments
  - daemonsets
  - replicasets
  - statefulsets
  verbs:
  - create
  - delete
  - get
  - list
  - patch
  - update
  - watch
- apiGroups:
  - monitoring.coreos.com
  resources:
  - servicemonitors
  verbs:
  - get
  - create
- apiGroups:
  - apps
  resourceNames:
  - imooc-operator
  resources:
  - deployments/finalizers
  verbs:
  - update
- apiGroups:
  - ""
  resources:
  - pods
  verbs:
  - get
- apiGroups:
  - apps
  resources:
  - replicasets
  - deployments
  verbs:
  - get
- apiGroups:
  - k8s.imooc.com
  resources:
  - '*'
  verbs:
  - create
  - delete
  - get
  - list
  - patch
  - update
  - watch
---
# Source: mychart/templates/role_binding.yaml
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: imooc-operator
subjects:
- kind: ServiceAccount
  name: imooc-operator
roleRef:
  kind: Role
  name: imooc-operator
  apiGroup: rbac.authorization.k8s.io
---
# Source: mychart/templates/operator.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: imooc-operator
spec:
  replicas: 1
  selector:
    matchLabels:
      name: imooc-operator
  template:
    metadata:
      labels:
        name: imooc-operator
    spec:
      serviceAccountName: imooc-operator
      containers:
        - name: imooc-operator
          # Replace this with the built image name
          # todo 替换为生成的镜像
          # kind load docker-image rubinus/imoocpod-operator
          # image: REPLACE_IMAGE
          # image: docker.io/rubinus/imoocpod-operator:v0.1
          # image: harbor-b.alauda.cn/tdsql/imoocpod-operator:origin_master-b18-7018120.0
          image: harbor-b.alauda.cn/imoocpod-operator:origin_master-b18-7018120.0
          command:
          - imooc-operator
          # 由于使用 kind 的缘故，imagePullPolicy 需要修改为 Never
          # imagePullPolicy: Always
          imagePullPolicy: Always
          env:
            - name: WATCH_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: OPERATOR_NAME
              value: "imooc-operator"



```



第一次出现 java 



https://github.com/lanzhiwang/strimzi-kafka-operator/commits/master?before=023e8cc92c0ab9e4d539da68a77df2f093c53770+3220&branch=master

Make the Travis build script compatible with sh in order to avoid pus…

